{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressive sensing example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num GPUs 1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import sigpy.mri as mr\n",
    "\n",
    "import sigpy as sp\n",
    "import sigpy.mri as mr\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from include import *\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "#from models import *\n",
    "#from utils.denoising_utils import *\n",
    "\n",
    "GPU = True\n",
    "if GPU == True:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crop_center(img,cropx,cropy):\n",
    "    #y,x = img.shape\n",
    "    y = img.shape[-2]\n",
    "    x = img.shape[-1]\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)\n",
    "    if len(img.shape) == 2:\n",
    "        return img[starty:starty+cropy,startx:startx+cropx]\n",
    "    if len(img.shape) == 3:\n",
    "        return img[0,starty:starty+cropy,startx:startx+cropx]\n",
    "\n",
    "path = './test_data/'\n",
    "img_name = \"poster\"\n",
    "#img_name = \"F16_GT\"\n",
    "#img_name = \"sf4_rgb\"\n",
    "#img_name  = 'library'\n",
    "img_path = path + img_name + \".png\"\n",
    "\n",
    "img_pil = Image.open(img_path)\n",
    "img_np = pil_to_np(img_pil)\n",
    "\n",
    "img_np_small = np.array([crop_center(img_np[0],128,128)])\n",
    "img_var = np_to_var(img_np_small).type(dtype)\n",
    "output_depth = img_np.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5461\n"
     ]
    }
   ],
   "source": [
    "X = img_var.view(-1, np.prod(img_var.shape) )\n",
    "n = X.shape[1]\n",
    "m = int(n/3)\n",
    "A = torch.empty(n,m).uniform_(-1, 1).type(dtype)\n",
    "A *= 1/np.sqrt(m)\n",
    "\n",
    "def forwardm(img_var):\n",
    "    X = img_var.view(-1 , np.prod(img_var.shape) ) \n",
    "    return torch.mm(X,A)\n",
    "\n",
    "measurement = forwardm(img_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DD reconstruction and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net_input(num_channels,w=128,h=128):\n",
    "    totalupsample = 2**len(num_channels)\n",
    "    width = int(128/totalupsample)\n",
    "    height = int(128/totalupsample)\n",
    "    shape = [1,num_channels[0], width, height]\n",
    "    net_input = Variable(torch.zeros(shape)).type(dtype)\n",
    "    net_input.data.uniform_()\n",
    "    net_input.data *= 1./10\n",
    "    return net_input\n",
    "\n",
    "def get_random_img(num_channels,ni=None):\n",
    "    if ni is None:\n",
    "        ni = get_net_input(num_channels)\n",
    "    net = decodernw(1,num_channels_up=num_channels,need_sigmoid=True).type(dtype)\n",
    "    print(\"generated random image with\", num_channels, \" network has \", num_param(net) )\n",
    "    return net(ni)\n",
    "\n",
    "def myimgshow(plt,img):\n",
    "    if(img.shape[0] == 1):\n",
    "        plt.imshow(np.clip(img[0],0,1),cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(np.clip(img.transpose(1, 2, 0),0,1))\n",
    "    plt.axis('off')    \n",
    "    \n",
    "def plot_img(img_ref): \n",
    "    fig = plt.figure(figsize = (15,15)) # create a 5 x 5 figure   \n",
    "    ax1 = fig.add_subplot(231)\n",
    "    ax1.imshow(img_ref,cmap='gray')\n",
    "    #ax1.set_title('Original image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "def init_weights(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            #m.weight.data.uniform_()\n",
    "            #torch.nn.init.xavier_uniform(m.weight)\n",
    "            #nn.init.uniform_(m.weight)\n",
    "            torch.nn.init.normal_(m.weight)\n",
    "\n",
    "def snr(x_hat,x_true):\n",
    "    x_hat = x_hat.flatten()\n",
    "    x_true = x_true.flatten()\n",
    "    mse= np.sum( np.square(x_hat-x_true) )\n",
    "    #snr_ = 10.*np.log(maxv**2/mse)/np.log(10.)\n",
    "    snr_ = mse / np.sum( np.square(x_true) )\n",
    "    return snr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd_recovery(measurement,img_var,num_channnels,num_iter=6000,apply_f=forwardm,ni=None):\n",
    "    net = decodernw(1,num_channels_up=num_channels,need_sigmoid=True).type(dtype)\n",
    "    #net.apply(init_weights)\n",
    "    mse_n, mse_t, ni, net = fit( num_channels=num_channels,\n",
    "                                net_input=ni,\n",
    "                        reg_noise_std=0.0,num_iter=num_iter,LR = 0.005,\n",
    "                        img_noisy_var=measurement.type(dtype),\n",
    "                        net=net,apply_f = apply_f,img_clean_var=img_var.type(dtype),\n",
    "                        upsample_mode='bilinear',\n",
    "                        )\n",
    "    print(num_param(net))\n",
    "    out_img_var = net( ni.type(dtype) )\n",
    "    return out_img_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example reconstruction\n",
    "\n",
    "This demonstrates that reconstruction with a deep decoder works well, but a deconvolutional decoder does not enable good reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  [1, 22, 8, 8]\n",
      "optimize with adam 0.005\n",
      "2662ation 05990    Train loss 0.000473  Actual loss 0.000794 Actual loss orig 0.000794 \n"
     ]
    }
   ],
   "source": [
    "k=22\n",
    "num_channels = [k]*4\n",
    "measurement = forwardm(img_var).type(dtype)\n",
    "out_img_var = dd_recovery(measurement,img_var,num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  [1, 6, 4, 4]\n",
      "optimize with adam 0.0025\n",
      "2946ation 04990    Train loss 0.000927  Actual loss 0.007248 Actual loss orig 0.007248 \n"
     ]
    }
   ],
   "source": [
    "def dconv_recovery(img_var):\n",
    "    measurement = forwardm(img_var).type(dtype)\n",
    "    num_channels = [6]*6\n",
    "    net = deconv_decoder(1,num_channels_up=num_channels,filter_size=4,stride=2,padding=1).type(dtype)\n",
    "    mse_n, mse_t, ni, net = fit( num_channels=num_channels,\n",
    "                        reg_noise_std=0.0,num_iter=5000,LR = 0.0025,\n",
    "                        img_noisy_var=measurement,\n",
    "                        net=net,apply_f = forwardm,img_clean_var=img_var.type(dtype),\n",
    "                        upsample_mode='deconv' )\n",
    "    print(num_param(net))\n",
    "    out_img_var = net( ni.type(dtype) )\n",
    "    return out_img_var\n",
    "\n",
    "out_img_dc_var = dconv_recovery(img_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(img_var.data.cpu().numpy()[0,0])\n",
    "#plot_img(out_img_var.data.cpu().numpy()[0,0])\n",
    "#plot_img(out_img_dc_var.data.cpu().numpy()[0,0])\n",
    "\n",
    "def savefig(filename,img):\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename,bbox_inches='tight')\n",
    "    \n",
    "savefig(img_name + \"_orig.png',img_var.data.cpu().numpy()[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressive sensing on random images\n",
    "\n",
    "Our main result shows that taking random linear measurements on the order of the number of parameters of the deep decoder is suffient for recovery is possible. In order to see whether that is also necessary and thus the number of parameters captures the complexity of the range space of the deep decoder, we conduct the following experiment to recover an image in the range of the deep decoder.\n",
    "\n",
    "In order to generate an image, we can in principle simply choose the coefficients of the deep decoder at random. However, for a deep decoder with a fixed number of parameters, this tends to generate simple images, in that often a deep decoder with much fewer coefficients can represent it well. To ensure that we generate a sufficiently complex image, we generate an image in the range of the generator by finding the best representation of noise with the deep decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 00380    Train loss 0.081738  Actual loss 0.081738 Actual loss orig 0.081738 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-dd51cd41d1d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# get random noise, and find approximation to it in the range of the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mimg_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mimg_approx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd_recovery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mapply_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#print(\"number useful variables / number observations\", num_param(net)/m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-73eb1c14ab50>\u001b[0m in \u001b[0;36mdd_recovery\u001b[0;34m(measurement, img_var, num_channnels, num_iter, apply_f, ni)\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mimg_noisy_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasurement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mapply_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_clean_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         \u001b[0mupsample_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                         )\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/convolutional_generators/include/fit.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(net, img_noisy_var, num_channels, img_clean_var, num_iter, LR, OPTIMIZER, opt_input, reg_noise_std, reg_noise_decayevery, mask_var, apply_f, lr_decay_epoch, net_input, net_input_gen, find_best, weight_decay, upsample_mode, totalupsample, loss_type, output_gradients, output_weights, show_images)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfind_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numpoints = 8\n",
    "ms = [ int(100*np.exp(5.5/numpoints*i)) for i in range(numpoints) ] #[100,200,,17000]\n",
    "print(ms)\n",
    "ks = [10,20,30,50,150,250]\n",
    "err = np.zeros((len(ms), len(ks)))\n",
    "\n",
    "numit = 10\n",
    "\n",
    "for q in range(numit):\n",
    "    for j,m in enumerate(ms):\n",
    "        for ell,k in enumerate(ks):\n",
    "            # generate input\n",
    "            num_channels = [k]*4\n",
    "            ni = get_net_input(num_channels)\n",
    "        \n",
    "            # get random noise, and find approximation to it in the range of the generator\n",
    "            img_var.data.uniform_()\n",
    "            img_approx = Variable(dd_recovery(img_var,img_var,num_channels,ni=ni,apply_f=None,num_iter=3000))\n",
    "\n",
    "            print(\"number useful variables / number observations\", (k**2*4 + k) /m)\n",
    "            print(\"number observations / number of variables\", m/n)\n",
    "            print(\"m,n,nump\",m,n,k**2*4 + k)\n",
    "            \n",
    "            # generate random matrix\n",
    "            A = 10*torch.empty(n,m).normal_(0, 1/np.sqrt(m)).type(dtype)\n",
    "            \n",
    "            def forwardm(img):\n",
    "                X = img.view(-1 , np.prod(img.shape) )\n",
    "                return torch.mm(X,A)\n",
    "\n",
    "            measurement = forwardm(img_approx).type(dtype)\n",
    "            out_img_var = dd_recovery(measurement,img_approx,num_channels,ni=ni,apply_f=forwardm,num_iter=10000)\n",
    "    \n",
    "            #plot_img(img_approx.data.cpu().numpy()[0,0])\n",
    "            #plot_img(out_img_var.data.cpu().numpy()[0,0])\n",
    "    \n",
    "            error = snr(out_img_var.data.cpu().numpy()[0] , img_approx.data.cpu().numpy()[0])\n",
    "            print(\"error: \", error, \"\\n\")\n",
    "            err[j,ell] += error/numit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot and save\n",
    "plt.xscale('log')\n",
    "for i,c in enumerate(['b','r','g','y','b']):\n",
    "    plt.plot(ms,err[:,i],c)\n",
    "plt.show()\n",
    "\n",
    "np.savetxt(\"csrandimg_\"+img_name+\".csv\", np.vstack([ np.array(ms) ,np.array(err).T]).T , delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressive sensing on a natural image for varying number of parameters and number of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number useful variables / number observations 4.1\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.000000  Actual loss 0.054498 Actual loss orig 0.054498    \n",
      "error:  0.6411835 \n",
      "\n",
      "number useful variables / number observations 16.2\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.000000  Actual loss 0.033326 Actual loss orig 0.033326    \n",
      "error:  0.3920929 \n",
      "\n",
      "number useful variables / number observations 36.3\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.003548  Actual loss 0.035698 Actual loss orig 0.035698    \n",
      "error:  0.41986442 \n",
      "\n",
      "number useful variables / number observations 100.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.000005  Actual loss 0.033560 Actual loss orig 0.033560    \n",
      "error:  0.3948583 \n",
      "\n",
      "number useful variables / number observations 901.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000582  Actual loss 0.063288 Actual loss orig 0.063288  6 \n",
      "error:  0.7447204 \n",
      "\n",
      "number useful variables / number observations 2502.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.001004  Actual loss 0.060623 Actual loss orig 0.060623    \n",
      "error:  0.7136777 \n",
      "\n",
      "number useful variables / number observations 2.0707070707070705\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.029782  Actual loss 0.048942 Actual loss orig 0.048942    \n",
      "error:  0.5756615 \n",
      "\n",
      "number useful variables / number observations 8.181818181818182\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.000204  Actual loss 0.033082 Actual loss orig 0.033082    \n",
      "error:  0.38919157 \n",
      "\n",
      "number useful variables / number observations 18.333333333333332\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.001137  Actual loss 0.022133 Actual loss orig 0.022133   \n",
      "error:  0.26040277 \n",
      "\n",
      "number useful variables / number observations 50.75757575757576\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.000030  Actual loss 0.018889 Actual loss orig 0.018889    \n",
      "error:  0.22225945 \n",
      "\n",
      "number useful variables / number observations 455.3030303030303\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.021891 Actual loss orig 0.021891    \n",
      "error:  0.25755167 \n",
      "\n",
      "number useful variables / number observations 1263.888888888889\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000000  Actual loss 0.024651 Actual loss orig 0.024651  2 \n",
      "error:  0.29003015 \n",
      "\n",
      "number useful variables / number observations 1.0379746835443038\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.772997  Actual loss 0.046697 Actual loss orig 0.046697   \n",
      "error:  0.5498007 \n",
      "\n",
      "number useful variables / number observations 4.10126582278481\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.007713  Actual loss 0.013636 Actual loss orig 0.013636   \n",
      "error:  0.16054828 \n",
      "\n",
      "number useful variables / number observations 9.189873417721518\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.056916  Actual loss 0.011249 Actual loss orig 0.011249   \n",
      "error:  0.132277 \n",
      "\n",
      "number useful variables / number observations 25.443037974683545\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.000083  Actual loss 0.009071 Actual loss orig 0.009071   \n",
      "error:  0.10652047 \n",
      "\n",
      "number useful variables / number observations 228.22784810126583\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.008327 Actual loss orig 0.008327   \n",
      "error:  0.09797426 \n",
      "\n",
      "number useful variables / number observations 633.5443037974684\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000045  Actual loss 0.007816 Actual loss orig 0.007816   \n",
      "error:  0.091959335 \n",
      "\n",
      "number useful variables / number observations 0.5216284987277354\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 6.004725  Actual loss 0.020873 Actual loss orig 0.020873   \n",
      "error:  0.24535525 \n",
      "\n",
      "number useful variables / number observations 2.0610687022900764\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.015020  Actual loss 0.009970 Actual loss orig 0.009970   \n",
      "error:  0.116922066 \n",
      "\n",
      "number useful variables / number observations 4.6183206106870225\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.001623  Actual loss 0.005242 Actual loss orig 0.005242   \n",
      "error:  0.061755925 \n",
      "\n",
      "number useful variables / number observations 12.786259541984732\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.082691  Actual loss 0.003806 Actual loss orig 0.003806   \n",
      "error:  0.044337917 \n",
      "\n",
      "number useful variables / number observations 114.69465648854961\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.006726  Actual loss 0.002260 Actual loss orig 0.002260   \n",
      "error:  0.026657952 \n",
      "\n",
      "number useful variables / number observations 318.38422391857506\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000000  Actual loss 0.002573 Actual loss orig 0.002573   \n",
      "error:  0.03026723 \n",
      "\n",
      "number useful variables / number observations 0.26214833759590794\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 4.499002  Actual loss 0.011418 Actual loss orig 0.011418   \n",
      "error:  0.13395393 \n",
      "\n",
      "number useful variables / number observations 1.0358056265984654\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.172140  Actual loss 0.002138 Actual loss orig 0.002138   \n",
      "error:  0.025357662 \n",
      "\n",
      "number useful variables / number observations 2.3209718670076724\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.031939  Actual loss 0.001897 Actual loss orig 0.001897   \n",
      "error:  0.022367412 \n",
      "\n",
      "number useful variables / number observations 6.4258312020460355\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13050tion 05990    Train loss 0.001594  Actual loss 0.001475 Actual loss orig 0.001475   \n",
      "error:  0.017505378 \n",
      "\n",
      "number useful variables / number observations 57.64066496163683\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000006  Actual loss 0.001183 Actual loss orig 0.001183   \n",
      "error:  0.013918822 \n",
      "\n",
      "number useful variables / number observations 160.00639386189258\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000003  Actual loss 0.000838 Actual loss orig 0.000838   \n",
      "error:  0.009861791 \n",
      "\n",
      "number useful variables / number observations 0.13183279742765272\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 1.141827  Actual loss 0.003731 Actual loss orig 0.003731  \n",
      "error:  0.04674172 \n",
      "\n",
      "number useful variables / number observations 0.5209003215434084\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.282276  Actual loss 0.001190 Actual loss orig 0.001190  \n",
      "error:  0.012794695 \n",
      "\n",
      "number useful variables / number observations 1.167202572347267\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.130407  Actual loss 0.000933 Actual loss orig 0.000933  \n",
      "error:  0.009473842 \n",
      "\n",
      "number useful variables / number observations 3.2315112540192925\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.013774  Actual loss 0.000492 Actual loss orig 0.000492  \n",
      "error:  0.0059002726 \n",
      "\n",
      "number useful variables / number observations 28.987138263665596\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.001607  Actual loss 0.000406 Actual loss orig 0.000406  \n",
      "error:  0.004803534 \n",
      "\n",
      "number useful variables / number observations 80.46623794212219\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.023422  Actual loss 0.000318 Actual loss orig 0.000318  \n",
      "error:  0.0036438527 \n",
      "\n",
      "number useful variables / number observations 0.0662786938247656\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.402703  Actual loss 0.002009 Actual loss orig 0.002009  \n",
      "error:  0.02311038 \n",
      "\n",
      "number useful variables / number observations 0.2618816682832202\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.077057  Actual loss 0.000550 Actual loss orig 0.000550  \n",
      "error:  0.0066921324 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.052310  Actual loss 0.000443 Actual loss orig 0.000443  \n",
      "error:  0.007269033 \n",
      "\n",
      "number useful variables / number observations 1.6246362754607178\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.029212  Actual loss 0.000290 Actual loss orig 0.000290  \n",
      "error:  0.0034014522 \n",
      "\n",
      "number useful variables / number observations 14.573229873908826\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.006645  Actual loss 0.000139 Actual loss orig 0.000139  \n",
      "error:  0.0028851817 \n",
      "\n",
      "number useful variables / number observations 40.45425153572583\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.006726  Actual loss 0.000140 Actual loss orig 0.000140  \n",
      "error:  0.0015745034 \n",
      "\n",
      "number useful variables / number observations 0.03332520523449565\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.440257  Actual loss 0.003736 Actual loss orig 0.003736  \n",
      "error:  0.043154318 \n",
      "\n",
      "number useful variables / number observations 0.13167520117044623\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.068124  Actual loss 0.000652 Actual loss orig 0.000652  \n",
      "error:  0.006409178 \n",
      "\n",
      "number useful variables / number observations 0.29504998780785174\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.052021  Actual loss 0.000498 Actual loss orig 0.000498  \n",
      "error:  0.004295183 \n",
      "\n",
      "number useful variables / number observations 0.8168739331870275\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.016087  Actual loss 0.000190 Actual loss orig 0.000190  \n",
      "error:  0.0022410497 \n",
      "\n",
      "number useful variables / number observations 7.327481102170203\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.011899  Actual loss 0.000145 Actual loss orig 0.000145  \n",
      "error:  0.0013948795 \n",
      "\n",
      "number useful variables / number observations 20.340567341298872\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.002908  Actual loss 0.000054 Actual loss orig 0.000054  \n",
      "error:  0.0010185948 \n",
      "\n",
      "number useful variables / number observations 4.1\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.006154  Actual loss 0.070133 Actual loss orig 0.070133    \n",
      "error:  0.8260117 \n",
      "\n",
      "number useful variables / number observations 16.2\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.000000  Actual loss 0.048628 Actual loss orig 0.048628    \n",
      "error:  0.57212347 \n",
      "\n",
      "number useful variables / number observations 36.3\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.000009  Actual loss 0.044756 Actual loss orig 0.044756    \n",
      "error:  0.52658904 \n",
      "\n",
      "number useful variables / number observations 100.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 2.170402  Actual loss 0.042252 Actual loss orig 0.042252    \n",
      "error:  0.4886905 \n",
      "\n",
      "number useful variables / number observations 901.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.039366 Actual loss orig 0.039366  2 \n",
      "error:  0.4631562 \n",
      "\n",
      "number useful variables / number observations 2502.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000227  Actual loss 0.055721 Actual loss orig 0.055721    \n",
      "error:  0.6567776 \n",
      "\n",
      "number useful variables / number observations 2.0707070707070705\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.057056  Actual loss 0.063485 Actual loss orig 0.063485    \n",
      "error:  0.74747854 \n",
      "\n",
      "number useful variables / number observations 8.181818181818182\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220ation 05990    Train loss 0.005683  Actual loss 0.031882 Actual loss orig 0.031882    \n",
      "error:  0.37517112 \n",
      "\n",
      "number useful variables / number observations 18.333333333333332\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.029595  Actual loss 0.032950 Actual loss orig 0.032950    \n",
      "error:  0.38877645 \n",
      "\n",
      "number useful variables / number observations 50.75757575757576\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.000000  Actual loss 0.020558 Actual loss orig 0.020558    \n",
      "error:  0.2418691 \n",
      "\n",
      "number useful variables / number observations 455.3030303030303\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000774  Actual loss 0.032278 Actual loss orig 0.032278  3 \n",
      "error:  0.3798278 \n",
      "\n",
      "number useful variables / number observations 1263.888888888889\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000002  Actual loss 0.055401 Actual loss orig 0.055401    \n",
      "error:  0.65181583 \n",
      "\n",
      "number useful variables / number observations 1.0379746835443038\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 2.876640  Actual loss 0.048591 Actual loss orig 0.048591   \n",
      "error:  0.5793536 \n",
      "\n",
      "number useful variables / number observations 4.10126582278481\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.195690  Actual loss 0.015207 Actual loss orig 0.015207   \n",
      "error:  0.17660728 \n",
      "\n",
      "number useful variables / number observations 9.189873417721518\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.036285  Actual loss 0.012037 Actual loss orig 0.012037   \n",
      "error:  0.14124855 \n",
      "\n",
      "number useful variables / number observations 25.443037974683545\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.000052  Actual loss 0.007601 Actual loss orig 0.007601   \n",
      "error:  0.089444585 \n",
      "\n",
      "number useful variables / number observations 228.22784810126583\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.010864 Actual loss orig 0.010864   \n",
      "error:  0.12782247 \n",
      "\n",
      "number useful variables / number observations 633.5443037974684\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000000  Actual loss 0.016684 Actual loss orig 0.016684   \n",
      "error:  0.19629863 \n",
      "\n",
      "number useful variables / number observations 0.5216284987277354\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 3.056517  Actual loss 0.012005 Actual loss orig 0.012005   \n",
      "error:  0.1378948 \n",
      "\n",
      "number useful variables / number observations 2.0610687022900764\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.060295  Actual loss 0.007072 Actual loss orig 0.007072   \n",
      "error:  0.08522157 \n",
      "\n",
      "number useful variables / number observations 4.6183206106870225\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.693652  Actual loss 0.003291 Actual loss orig 0.003291   \n",
      "error:  0.034904834 \n",
      "\n",
      "number useful variables / number observations 0.26214833759590794\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 2.782465  Actual loss 0.007008 Actual loss orig 0.007008   \n",
      "error:  0.080486335 \n",
      "\n",
      "number useful variables / number observations 1.0358056265984654\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.323319  Actual loss 0.002021 Actual loss orig 0.002021   \n",
      "error:  0.022116128 \n",
      "\n",
      "number useful variables / number observations 2.3209718670076724\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.100800  Actual loss 0.002025 Actual loss orig 0.002025   \n",
      "error:  0.023039486 \n",
      "\n",
      "number useful variables / number observations 6.4258312020460355\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.001096  Actual loss 0.001667 Actual loss orig 0.001667   \n",
      "error:  0.019594334 \n",
      "\n",
      "number useful variables / number observations 57.64066496163683\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000506  Actual loss 0.001121 Actual loss orig 0.001121   \n",
      "error:  0.0131714055 \n",
      "\n",
      "number useful variables / number observations 160.00639386189258\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000010  Actual loss 0.001114 Actual loss orig 0.001114   \n",
      "error:  0.013115213 \n",
      "\n",
      "number useful variables / number observations 0.13183279742765272\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 1.311004  Actual loss 0.004066 Actual loss orig 0.004066  \n",
      "error:  0.048079967 \n",
      "\n",
      "number useful variables / number observations 0.5209003215434084\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.201574  Actual loss 0.001174 Actual loss orig 0.001174  \n",
      "error:  0.013286604 \n",
      "\n",
      "number useful variables / number observations 1.167202572347267\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.040759  Actual loss 0.000714 Actual loss orig 0.000714  \n",
      "error:  0.009116125 \n",
      "\n",
      "number useful variables / number observations 3.2315112540192925\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.345712  Actual loss 0.001089 Actual loss orig 0.001089  \n",
      "error:  0.019096125 \n",
      "\n",
      "number useful variables / number observations 28.987138263665596\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.003248  Actual loss 0.000435 Actual loss orig 0.000435  \n",
      "error:  0.0050425166 \n",
      "\n",
      "number useful variables / number observations 80.46623794212219\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.049056  Actual loss 0.000349 Actual loss orig 0.000349  \n",
      "error:  0.0037330664 \n",
      "\n",
      "number useful variables / number observations 0.0662786938247656\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 1.031005  Actual loss 0.005131 Actual loss orig 0.005131  \n",
      "error:  0.057608653 \n",
      "\n",
      "number useful variables / number observations 0.2618816682832202\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.102721  Actual loss 0.000678 Actual loss orig 0.000678  \n",
      "error:  0.008037967 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 05420    Train loss 0.052621  Actual loss 0.000483 Actual loss orig 0.000483  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13050tion 05990    Train loss 0.001616  Actual loss 0.001449 Actual loss orig 0.001449 \n",
      "error:  0.017005272 \n",
      "\n",
      "number useful variables / number observations 57.64066496163683\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.001779  Actual loss 0.001084 Actual loss orig 0.001084   \n",
      "error:  0.0127444705 \n",
      "\n",
      "number useful variables / number observations 160.00639386189258\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.116646  Actual loss 0.000813 Actual loss orig 0.000813   \n",
      "error:  0.008993751 \n",
      "\n",
      "number useful variables / number observations 0.13183279742765272\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.993266  Actual loss 0.003244 Actual loss orig 0.003244  \n",
      "error:  0.038090125 \n",
      "\n",
      "number useful variables / number observations 0.5209003215434084\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.179334  Actual loss 0.001118 Actual loss orig 0.001118  \n",
      "error:  0.012817167 \n",
      "\n",
      "number useful variables / number observations 1.167202572347267\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.079193  Actual loss 0.000603 Actual loss orig 0.000603  \n",
      "error:  0.006655227 \n",
      "\n",
      "number useful variables / number observations 3.2315112540192925\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.007422  Actual loss 0.000606 Actual loss orig 0.000606  \n",
      "error:  0.00865054 \n",
      "\n",
      "number useful variables / number observations 28.987138263665596\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.021106  Actual loss 0.000374 Actual loss orig 0.000374  \n",
      "error:  0.004176751 \n",
      "\n",
      "number useful variables / number observations 80.46623794212219\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.020406  Actual loss 0.000308 Actual loss orig 0.000308  \n",
      "error:  0.0036018584 \n",
      "\n",
      "number useful variables / number observations 0.0662786938247656\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.259350  Actual loss 0.001208 Actual loss orig 0.001208  \n",
      "error:  0.017864803 \n",
      "\n",
      "number useful variables / number observations 0.2618816682832202\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.089883  Actual loss 0.000651 Actual loss orig 0.000651  \n",
      "error:  0.008039919 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.047071  Actual loss 0.000392 Actual loss orig 0.000392  \n",
      "error:  0.0044974266 \n",
      "\n",
      "number useful variables / number observations 1.6246362754607178\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.025281  Actual loss 0.000306 Actual loss orig 0.000306  \n",
      "error:  0.00463421 \n",
      "\n",
      "number useful variables / number observations 14.573229873908826\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.008440  Actual loss 0.000147 Actual loss orig 0.000147  \n",
      "error:  0.0016140231 \n",
      "\n",
      "number useful variables / number observations 40.45425153572583\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.007036  Actual loss 0.000147 Actual loss orig 0.000147  \n",
      "error:  0.0016739217 \n",
      "\n",
      "number useful variables / number observations 0.03332520523449565\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 01430    Train loss 0.426443  Actual loss 0.003624 Actual loss orig 0.003624  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220ation 05990    Train loss 0.202846  Actual loss 0.001192 Actual loss orig 0.001192 \n",
      "error:  0.015404466 \n",
      "\n",
      "number useful variables / number observations 1.167202572347267\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.227491  Actual loss 0.000952 Actual loss orig 0.000952  \n",
      "error:  0.008411814 \n",
      "\n",
      "number useful variables / number observations 3.2315112540192925\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.013586  Actual loss 0.000568 Actual loss orig 0.000568  \n",
      "error:  0.0070038205 \n",
      "\n",
      "number useful variables / number observations 28.987138263665596\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.011097  Actual loss 0.000321 Actual loss orig 0.000321  \n",
      "error:  0.003805382 \n",
      "\n",
      "number useful variables / number observations 80.46623794212219\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.013059  Actual loss 0.000305 Actual loss orig 0.000305  \n",
      "error:  0.0035008886 \n",
      "\n",
      "number useful variables / number observations 0.0662786938247656\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.810333  Actual loss 0.003944 Actual loss orig 0.003944  \n",
      "error:  0.048131704 \n",
      "\n",
      "number useful variables / number observations 0.2618816682832202\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.079000  Actual loss 0.000527 Actual loss orig 0.000527  \n",
      "error:  0.0061393445 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.055625  Actual loss 0.000442 Actual loss orig 0.000442  \n",
      "error:  0.0048454157 \n",
      "\n",
      "number useful variables / number observations 1.6246362754607178\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.014828  Actual loss 0.000210 Actual loss orig 0.000210  \n",
      "error:  0.0028285615 \n",
      "\n",
      "number useful variables / number observations 14.573229873908826\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.005747  Actual loss 0.000143 Actual loss orig 0.000143  \n",
      "error:  0.0016317456 \n",
      "\n",
      "number useful variables / number observations 40.45425153572583\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.022143  Actual loss 0.000182 Actual loss orig 0.000182  \n",
      "error:  0.009744174 \n",
      "\n",
      "number useful variables / number observations 0.03332520523449565\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.326524  Actual loss 0.002696 Actual loss orig 0.002696  \n",
      "error:  0.030300388 \n",
      "\n",
      "number useful variables / number observations 0.13167520117044623\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.057586  Actual loss 0.000565 Actual loss orig 0.000565  \n",
      "error:  0.006602525 \n",
      "\n",
      "number useful variables / number observations 0.29504998780785174\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.029578  Actual loss 0.000328 Actual loss orig 0.000328  \n",
      "error:  0.005691415 \n",
      "\n",
      "number useful variables / number observations 0.8168739331870275\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.009018  Actual loss 0.000133 Actual loss orig 0.000133  \n",
      "error:  0.001788746 \n",
      "\n",
      "number useful variables / number observations 7.327481102170203\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 04100    Train loss 0.006326  Actual loss 0.000100 Actual loss orig 0.000100  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114150ion 05990    Train loss 0.007226  Actual loss 0.000367 Actual loss orig 0.000367 \n",
      "error:  0.0042342395 \n",
      "\n",
      "number useful variables / number observations 80.46623794212219\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.002062  Actual loss 0.000350 Actual loss orig 0.000350  \n",
      "error:  0.004093266 \n",
      "\n",
      "number useful variables / number observations 0.0662786938247656\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.594539  Actual loss 0.002839 Actual loss orig 0.002839  \n",
      "error:  0.0346259 \n",
      "\n",
      "number useful variables / number observations 0.2618816682832202\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.074455  Actual loss 0.000513 Actual loss orig 0.000513  \n",
      "error:  0.0057777786 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.038150  Actual loss 0.000356 Actual loss orig 0.000356  \n",
      "error:  0.005241905 \n",
      "\n",
      "number useful variables / number observations 1.6246362754607178\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.019663  Actual loss 0.000266 Actual loss orig 0.000266  \n",
      "error:  0.002936538 \n",
      "\n",
      "number useful variables / number observations 14.573229873908826\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.004240  Actual loss 0.000133 Actual loss orig 0.000133  \n",
      "error:  0.0015619829 \n",
      "\n",
      "number useful variables / number observations 40.45425153572583\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.010733  Actual loss 0.000184 Actual loss orig 0.000184  \n",
      "error:  0.0025409493 \n",
      "\n",
      "number useful variables / number observations 0.03332520523449565\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.241933  Actual loss 0.002019 Actual loss orig 0.002019  \n",
      "error:  0.024274074 \n",
      "\n",
      "number useful variables / number observations 0.13167520117044623\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.044764  Actual loss 0.000435 Actual loss orig 0.000435  \n",
      "error:  0.005413741 \n",
      "\n",
      "number useful variables / number observations 0.29504998780785174\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.027525  Actual loss 0.000289 Actual loss orig 0.000289  \n",
      "error:  0.003611036 \n",
      "\n",
      "number useful variables / number observations 0.8168739331870275\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.017793  Actual loss 0.000208 Actual loss orig 0.000208  \n",
      "error:  0.0024804752 \n",
      "\n",
      "number useful variables / number observations 7.327481102170203\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.031368  Actual loss 0.000292 Actual loss orig 0.000292  \n",
      "error:  0.0032905212 \n",
      "\n",
      "number useful variables / number observations 20.340567341298872\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.006448  Actual loss 0.000097 Actual loss orig 0.000097  \n",
      "error:  0.0014188214 \n",
      "\n",
      "number useful variables / number observations 4.1\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.000000  Actual loss 0.055487 Actual loss orig 0.055487    \n",
      "error:  0.6528282 \n",
      "\n",
      "number useful variables / number observations 16.2\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.000000  Actual loss 0.044192 Actual loss orig 0.044192    \n",
      "error:  0.51992995 \n",
      "\n",
      "number useful variables / number observations 36.3\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.000215  Actual loss 0.032228 Actual loss orig 0.032228    \n",
      "error:  0.37915638 \n",
      "\n",
      "number useful variables / number observations 100.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.143720  Actual loss 0.033826 Actual loss orig 0.033826    \n",
      "error:  0.39731207 \n",
      "\n",
      "number useful variables / number observations 901.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.054691 Actual loss orig 0.054691  9 \n",
      "error:  0.6434602 \n",
      "\n",
      "number useful variables / number observations 2502.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000003  Actual loss 0.051156 Actual loss orig 0.051156    \n",
      "error:  0.60188323 \n",
      "\n",
      "number useful variables / number observations 2.0707070707070705\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.011535  Actual loss 0.053751 Actual loss orig 0.053751   \n",
      "error:  0.6323897 \n",
      "\n",
      "number useful variables / number observations 8.181818181818182\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000264  Actual loss 0.016666 Actual loss orig 0.016666   \n",
      "error:  0.1960289 \n",
      "\n",
      "number useful variables / number observations 1263.888888888889\n",
      "number observations / number of variables 0.0120849609375\n",
      "m,n,nump 198 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000000  Actual loss 0.030294 Actual loss orig 0.030294    \n",
      "error:  0.3564159 \n",
      "\n",
      "number useful variables / number observations 1.0379746835443038\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 3.449120  Actual loss 0.045993 Actual loss orig 0.045993   \n",
      "error:  0.55573034 \n",
      "\n",
      "number useful variables / number observations 4.10126582278481\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.001411  Actual loss 0.014535 Actual loss orig 0.014535   \n",
      "error:  0.17110264 \n",
      "\n",
      "number useful variables / number observations 9.189873417721518\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.000009  Actual loss 0.012849 Actual loss orig 0.012849   \n",
      "error:  0.15116797 \n",
      "\n",
      "number useful variables / number observations 25.443037974683545\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.000000  Actual loss 0.009083 Actual loss orig 0.009083   \n",
      "error:  0.1068628 \n",
      "\n",
      "number useful variables / number observations 228.22784810126583\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.008836 Actual loss orig 0.008836   \n",
      "error:  0.10395332 \n",
      "\n",
      "number useful variables / number observations 633.5443037974684\n",
      "number observations / number of variables 0.02410888671875\n",
      "m,n,nump 395 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315250ion 05990    Train loss 0.000024  Actual loss 0.007619 Actual loss orig 0.007619   \n",
      "error:  0.089692555 \n",
      "\n",
      "number useful variables / number observations 0.5216284987277354\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 3.144005  Actual loss 0.015468 Actual loss orig 0.015468   \n",
      "error:  0.18170722 \n",
      "\n",
      "number useful variables / number observations 2.0610687022900764\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.048589  Actual loss 0.006217 Actual loss orig 0.006217   \n",
      "error:  0.07385596 \n",
      "\n",
      "number useful variables / number observations 4.6183206106870225\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.001155  Actual loss 0.004393 Actual loss orig 0.004393   \n",
      "error:  0.0517126 \n",
      "\n",
      "number useful variables / number observations 12.786259541984732\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.003298  Actual loss 0.003585 Actual loss orig 0.003585   \n",
      "error:  0.04221524 \n",
      "\n",
      "number useful variables / number observations 114.69465648854961\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.002764 Actual loss orig 0.002764   \n",
      "error:  0.032519836 \n",
      "\n",
      "number useful variables / number observations 318.38422391857506\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000001  Actual loss 0.002341 Actual loss orig 0.002341   \n",
      "error:  0.02753912 \n",
      "\n",
      "number useful variables / number observations 0.26214833759590794\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 6.021989  Actual loss 0.015300 Actual loss orig 0.015300   \n",
      "error:  0.18186647 \n",
      "\n",
      "number useful variables / number observations 1.0358056265984654\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.146424  Actual loss 0.002488 Actual loss orig 0.002488   \n",
      "error:  0.029511042 \n",
      "\n",
      "number useful variables / number observations 2.3209718670076724\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.190421  Actual loss 0.001949 Actual loss orig 0.001949   \n",
      "error:  0.022210442 \n",
      "\n",
      "number useful variables / number observations 6.4258312020460355\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.012407  Actual loss 0.001298 Actual loss orig 0.001298   \n",
      "error:  0.015232595 \n",
      "\n",
      "number useful variables / number observations 57.64066496163683\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.297880  Actual loss 0.001287 Actual loss orig 0.001287   \n",
      "error:  0.025714166 \n",
      "\n",
      "number useful variables / number observations 160.00639386189258\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 00860    Train loss 0.023387  Actual loss 0.001115 Actual loss orig 0.001115   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4830ation 05990    Train loss 0.015608  Actual loss 0.005137 Actual loss orig 0.005137 \n",
      "error:  0.06021849 \n",
      "\n",
      "number useful variables / number observations 12.786259541984732\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.000000  Actual loss 0.003665 Actual loss orig 0.003665   \n",
      "error:  0.043115254 \n",
      "\n",
      "number useful variables / number observations 114.69465648854961\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000132  Actual loss 0.002561 Actual loss orig 0.002561   \n",
      "error:  0.030128963 \n",
      "\n",
      "number useful variables / number observations 318.38422391857506\n",
      "number observations / number of variables 0.0479736328125\n",
      "m,n,nump 786 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000096  Actual loss 0.002248 Actual loss orig 0.002248   \n",
      "error:  0.02653332 \n",
      "\n",
      "number useful variables / number observations 0.26214833759590794\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 4.686183  Actual loss 0.011530 Actual loss orig 0.011530   \n",
      "error:  0.13468736 \n",
      "\n",
      "number useful variables / number observations 1.0358056265984654\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.139196  Actual loss 0.002307 Actual loss orig 0.002307   \n",
      "error:  0.026538827 \n",
      "\n",
      "number useful variables / number observations 2.3209718670076724\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.023941  Actual loss 0.001855 Actual loss orig 0.001855   \n",
      "error:  0.022084596 \n",
      "\n",
      "number useful variables / number observations 6.4258312020460355\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.182921  Actual loss 0.001472 Actual loss orig 0.001472   \n",
      "error:  0.017251723 \n",
      "\n",
      "number useful variables / number observations 57.64066496163683\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000518  Actual loss 0.001041 Actual loss orig 0.001041   \n",
      "error:  0.01226702 \n",
      "\n",
      "number useful variables / number observations 160.00639386189258\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.000247  Actual loss 0.000722 Actual loss orig 0.000722   \n",
      "error:  0.008569957 \n",
      "\n",
      "number useful variables / number observations 0.13183279742765272\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 1.127599  Actual loss 0.003693 Actual loss orig 0.003693  \n",
      "error:  0.043963242 \n",
      "\n",
      "number useful variables / number observations 0.5209003215434084\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.222562  Actual loss 0.001194 Actual loss orig 0.001194  \n",
      "error:  0.013011604 \n",
      "\n",
      "number useful variables / number observations 1.167202572347267\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.067958  Actual loss 0.000725 Actual loss orig 0.000725  \n",
      "error:  0.00820145 \n",
      "\n",
      "number useful variables / number observations 3.2315112540192925\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.033272  Actual loss 0.000630 Actual loss orig 0.000630  \n",
      "error:  0.0074456027 \n",
      "\n",
      "number useful variables / number observations 28.987138263665596\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.011204  Actual loss 0.000370 Actual loss orig 0.000370  \n",
      "error:  0.0043958467 \n",
      "\n",
      "number useful variables / number observations 80.46623794212219\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.022992  Actual loss 0.000304 Actual loss orig 0.000304  \n",
      "error:  0.003404131 \n",
      "\n",
      "number useful variables / number observations 0.0662786938247656\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.596029  Actual loss 0.002977 Actual loss orig 0.002977  \n",
      "error:  0.033392075 \n",
      "\n",
      "number useful variables / number observations 0.2618816682832202\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.065023  Actual loss 0.000446 Actual loss orig 0.000446  \n",
      "error:  0.005120579 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 00560    Train loss 0.125151  Actual loss 0.000772 Actual loss orig 0.000772  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114150ion 05990    Train loss 0.000666  Actual loss 0.000889 Actual loss orig 0.000889 \n",
      "error:  0.010821622 \n",
      "\n",
      "number useful variables / number observations 160.00639386189258\n",
      "number observations / number of variables 0.095458984375\n",
      "m,n,nump 1564 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.006256  Actual loss 0.000871 Actual loss orig 0.000871   \n",
      "error:  0.010538536 \n",
      "\n",
      "number useful variables / number observations 0.13183279742765272\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 2.025880  Actual loss 0.006626 Actual loss orig 0.006626  \n",
      "error:  0.07790779 \n",
      "\n",
      "number useful variables / number observations 0.5209003215434084\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.144795  Actual loss 0.000908 Actual loss orig 0.000908  \n",
      "error:  0.010423008 \n",
      "\n",
      "number useful variables / number observations 1.167202572347267\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.040229  Actual loss 0.000563 Actual loss orig 0.000563  \n",
      "error:  0.007748881 \n",
      "\n",
      "number useful variables / number observations 3.2315112540192925\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.022969  Actual loss 0.000554 Actual loss orig 0.000554  \n",
      "error:  0.006382266 \n",
      "\n",
      "number useful variables / number observations 28.987138263665596\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.011511  Actual loss 0.000481 Actual loss orig 0.000481  \n",
      "error:  0.005550924 \n",
      "\n",
      "number useful variables / number observations 80.46623794212219\n",
      "number observations / number of variables 0.1898193359375\n",
      "m,n,nump 3110 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.014987  Actual loss 0.000332 Actual loss orig 0.000332  \n",
      "error:  0.0038870831 \n",
      "\n",
      "number useful variables / number observations 0.0662786938247656\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.542515  Actual loss 0.002682 Actual loss orig 0.002682  \n",
      "error:  0.031138534 \n",
      "\n",
      "number useful variables / number observations 0.2618816682832202\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.106096  Actual loss 0.000652 Actual loss orig 0.000652  \n",
      "error:  0.0068037533 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.048773  Actual loss 0.000440 Actual loss orig 0.000440  \n",
      "error:  0.005411788 \n",
      "\n",
      "number useful variables / number observations 1.6246362754607178\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.015939  Actual loss 0.000233 Actual loss orig 0.000233  \n",
      "error:  0.0028635939 \n",
      "\n",
      "number useful variables / number observations 14.573229873908826\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.003747  Actual loss 0.000141 Actual loss orig 0.000141  \n",
      "error:  0.0016570381 \n",
      "\n",
      "number useful variables / number observations 40.45425153572583\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.005255  Actual loss 0.000126 Actual loss orig 0.000126  \n",
      "error:  0.0017168666 \n",
      "\n",
      "number useful variables / number observations 0.03332520523449565\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.209682  Actual loss 0.001754 Actual loss orig 0.001754  \n",
      "error:  0.021850785 \n",
      "\n",
      "number useful variables / number observations 0.13167520117044623\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.054679  Actual loss 0.000523 Actual loss orig 0.000523  \n",
      "error:  0.0055230707 \n",
      "\n",
      "number useful variables / number observations 0.29504998780785174\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 03530    Train loss 0.030086  Actual loss 0.000329 Actual loss orig 0.000329  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220ation 05990    Train loss 0.103118  Actual loss 0.000667 Actual loss orig 0.000667 \n",
      "error:  0.0072002118 \n",
      "\n",
      "number useful variables / number observations 0.5868089233753637\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.083360  Actual loss 0.000559 Actual loss orig 0.000559  \n",
      "error:  0.0051842355 \n",
      "\n",
      "number useful variables / number observations 1.6246362754607178\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.028692  Actual loss 0.000260 Actual loss orig 0.000260  \n",
      "error:  0.0026105214 \n",
      "\n",
      "number useful variables / number observations 14.573229873908826\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.034056  Actual loss 0.000252 Actual loss orig 0.000252  \n",
      "error:  0.002909363 \n",
      "\n",
      "number useful variables / number observations 40.45425153572583\n",
      "number observations / number of variables 0.3775634765625\n",
      "m,n,nump 6186 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.004511  Actual loss 0.000124 Actual loss orig 0.000124  \n",
      "error:  0.0018112712 \n",
      "\n",
      "number useful variables / number observations 0.03332520523449565\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.270300  Actual loss 0.002236 Actual loss orig 0.002236  \n",
      "error:  0.025133844 \n",
      "\n",
      "number useful variables / number observations 0.13167520117044623\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.091762  Actual loss 0.000818 Actual loss orig 0.000818  \n",
      "error:  0.0066469857 \n",
      "\n",
      "number useful variables / number observations 0.29504998780785174\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.031049  Actual loss 0.000330 Actual loss orig 0.000330  \n",
      "error:  0.0034488488 \n",
      "\n",
      "number useful variables / number observations 0.8168739331870275\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.011449  Actual loss 0.000157 Actual loss orig 0.000157  \n",
      "error:  0.0030303318 \n",
      "\n",
      "number useful variables / number observations 7.327481102170203\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.004590  Actual loss 0.000075 Actual loss orig 0.000075  \n",
      "error:  0.0008940126 \n",
      "\n",
      "number useful variables / number observations 20.340567341298872\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.002587  Actual loss 0.000047 Actual loss orig 0.000047  \n",
      "error:  0.000635242 \n",
      "\n",
      "number useful variables / number observations 4.1\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 410\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "610ration 05990    Train loss 0.000000  Actual loss 0.063342 Actual loss orig 0.063342    \n",
      "error:  0.7452347 \n",
      "\n",
      "number useful variables / number observations 16.2\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 1620\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "2220ation 05990    Train loss 0.008014  Actual loss 0.042673 Actual loss orig 0.042673    \n",
      "error:  0.50183356 \n",
      "\n",
      "number useful variables / number observations 36.3\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 3630\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "4830ation 05990    Train loss 0.011637  Actual loss 0.030394 Actual loss orig 0.030394    \n",
      "error:  0.35729542 \n",
      "\n",
      "number useful variables / number observations 100.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.017217  Actual loss 0.035449 Actual loss orig 0.035449    \n",
      "error:  0.4170355 \n",
      "\n",
      "number useful variables / number observations 901.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.000000  Actual loss 0.033275 Actual loss orig 0.033275  9 \n",
      "error:  0.3914894 \n",
      "\n",
      "number useful variables / number observations 2502.5\n",
      "number observations / number of variables 0.006103515625\n",
      "m,n,nump 100 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "Iteration 02490    Train loss 0.027180  Actual loss 0.072018 Actual loss orig 0.072018    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4830ation 05990    Train loss 0.029336  Actual loss 0.000326 Actual loss orig 0.000326 \n",
      "error:  0.0034641582 \n",
      "\n",
      "number useful variables / number observations 0.8168739331870275\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 10050\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "13050tion 05990    Train loss 0.012945  Actual loss 0.000183 Actual loss orig 0.000183  \n",
      "error:  0.002170161 \n",
      "\n",
      "number useful variables / number observations 7.327481102170203\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 90150\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "114150ion 05990    Train loss 0.006862  Actual loss 0.000090 Actual loss orig 0.000090  \n",
      "error:  0.0009669987 \n",
      "\n",
      "number useful variables / number observations 20.340567341298872\n",
      "number observations / number of variables 0.75091552734375\n",
      "m,n,nump 12303 16384 250250\n",
      "input provided\n",
      "optimize with adam 0.005\n",
      "315250ion 05990    Train loss 0.012107  Actual loss 0.000138 Actual loss orig 0.000138  \n",
      "error:  0.0013842871 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get a small image\n",
    "img_name = \"poster\" # \"F16_GT\"\n",
    "img_path = path + img_name + \".png\"\n",
    "img_pil = Image.open(img_path)\n",
    "img_np = pil_to_np(img_pil)\n",
    "img_np_small = np.array([crop_center(img_np[0],128,128)])\n",
    "img_var = np_to_var(img_np_small).type(dtype)\n",
    "\n",
    "numpoints = 8\n",
    "ms = [ int(100*np.exp(5.5/numpoints*i)) for i in range(numpoints) ] #[100,200,,17000]\n",
    "ks = [10,20,30,50,150,250]\n",
    "\n",
    "err2 = np.zeros((len(ms), len(ks)))\n",
    "\n",
    "numit = 10\n",
    "\n",
    "for q in range(numit):\n",
    "    for j,m in enumerate(ms):\n",
    "        for ell,k in enumerate(ks):\n",
    "        \n",
    "            # generate fixed input\n",
    "            num_channels = [k]*4\n",
    "            ni = get_net_input(num_channels)\n",
    "        \n",
    "            #print(\"number useful variables / number observations\", num_param(net)/m)\n",
    "            print(\"number useful variables / number observations\", (k**2*4 + k) /m)\n",
    "            print(\"number observations / number of variables\", m/n)\n",
    "            print(\"m,n,nump\",m,n,k**2*4 + k)\n",
    "\n",
    "            A = 10*torch.empty(n,m).normal_(0, 1/np.sqrt(m)).type(dtype)\n",
    "            \n",
    "            def forwardm(img):\n",
    "                X = img.view(-1 , np.prod(img.shape) )\n",
    "                return torch.mm(X,A)\n",
    "            \n",
    "            # take measurement of original image\n",
    "            measurement = forwardm(img_var).type(dtype)\n",
    "            out_img_var = dd_recovery(measurement,img_var,num_channels,ni=ni,apply_f=forwardm,num_iter=6000)\n",
    "        \n",
    "            error = snr(out_img_var.data.cpu().numpy()[0] , img_var.data.cpu().numpy()[0])\n",
    "            print(\"error: \", error, \"\\n\")\n",
    "            err2[j,ell] += error/numit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot and save\n",
    "plt.xscale('log')\n",
    "for i,c in enumerate(['b','r','g','y','b','o']):\n",
    "    plt.plot(ms,err2[:,i],c)\n",
    "plt.show()\n",
    "\n",
    "np.savetxt(\"csf16img_\"+img_name+\".csv\", np.vstack([ np.array(ms) ,np.array(err2).T]).T , delimiter=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
